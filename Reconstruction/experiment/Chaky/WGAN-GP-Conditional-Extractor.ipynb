{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f02d41a96f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "All parameters are from https://arxiv.org/abs/1701.07875 and https://arxiv.org/abs/1704.00028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"data/nice_six/\"\n",
    "num_epochs = 5\n",
    "workers = 4\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "num_channel = 3 #<---change \n",
    "z_dim = 100\n",
    "gen_dim = 64\n",
    "dis_dim = 64\n",
    "ngpu = 1\n",
    "lr = 1e-4 #<-----changed\n",
    "dis_iter = 2\n",
    "lambda_gp = 10\n",
    "num_classes = 4\n",
    "embed_size = 100 #<---follow the paper\n",
    "feature_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n",
      "Min (should be -1):  tensor(-1.)\n",
      "Max (should be 1):  tensor(1.)\n",
      "X shape: (batch, C, H, W)   torch.Size([128, 3, 64, 64])\n",
      "Label shape: (batch)   torch.Size([128])\n",
      "Label unique values: (batch)   tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),  \n",
    "                               transforms.Normalize([0.5 for _ in range(num_channel)], [0.5 for _ in range(num_channel)]), \n",
    "#                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                           ]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "print(\"Device: \", device)\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "print(\"Min (should be -1): \", real_batch[0].min())\n",
    "print(\"Max (should be 1): \", real_batch[0].max())\n",
    "print(\"X shape: (batch, C, H, W)  \", real_batch[0].shape)\n",
    "print(\"Label shape: (batch)  \", real_batch[1].shape)\n",
    "print(\"Label unique values: (batch)  \", real_batch[1].unique())\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(model):\n",
    "    for m in model.modules(): #loop all layers in that model\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticImageExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    This class expected image as input with size (64x64x3)\n",
    "    \"\"\"\n",
    "    def __init__(self, ngpu, pretrain=False):\n",
    "        self.ngpu = ngpu\n",
    "        super(SemanticImageExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Alex1\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Alex2\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Alex3\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Alex4\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Alex5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        # return the same number of features but change width and height of img\n",
    "        if(pretrain):\n",
    "            import torchvision\n",
    "            ori_alex = torchvision.models.alexnet(pretrained = True)\n",
    "            ori_weight = ori_alex.state_dict()\n",
    "            ori_weight.pop('classifier.1.weight')\n",
    "            ori_weight.pop('classifier.1.bias')\n",
    "            ori_weight.pop('classifier.4.weight')\n",
    "            ori_weight.pop('classifier.4.bias')\n",
    "            ori_weight.pop('classifier.6.weight')\n",
    "            ori_weight.pop('classifier.6.bias')\n",
    "            self.load_state_dict(ori_weight)\n",
    "            del(ori_alex)\n",
    "            del(ori_weight)\n",
    "        # finally\n",
    "        self._add_classifier(num_classes, feature_size)\n",
    "    def _add_classifier(self, num_classes, feature_size):\n",
    "        self.fc06 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc07 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, feature_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc08 = nn.Sequential(\n",
    "            nn.Linear(feature_size, num_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc06(x)\n",
    "        semantic_features = self.fc07(x)\n",
    "        p_label = self.fc08(semantic_features)\n",
    "        return semantic_features, p_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticImageExtractor(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (fc06): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc07): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=4096, out_features=200, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc08): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the image feature extractor\n",
    "netIE = SemanticImageExtractor(ngpu, True).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "# netIE.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Image Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's write a test function to test whether our image extractor are always correctly coded\n",
    "def test():\n",
    "    batch, num_channels, H, W = 8, 3, 64, 64\n",
    "    x = torch.randn((batch, num_channels, H, W))\n",
    "    labels = torch.randint(0, 4, (batch,), dtype=torch.float)\n",
    "    labels = labels.long() #<----pytorch expect the labels to be long\n",
    "    \n",
    "    #testing discriminator\n",
    "    netIE = SemanticImageExtractor(ngpu, True)\n",
    "#     netIE.apply(weights_init)\n",
    "    latent, labels = netIE(x)\n",
    "    assert latent.shape == (batch, feature_size) #(8, 200)\n",
    "    assert labels.shape == (batch, num_classes) #(8, 4)\n",
    "    \n",
    "# test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Image Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Epoch: 4; Accuracy: 98.51851851851852; Loss: 0.0009606179881978918"
     ]
    }
   ],
   "source": [
    "#optimizer\n",
    "# optimizerIE = optim.Adam(netIE.parameters(), lr=lr, betas=(0.5, 0.999))  \n",
    "optimizerIE = optim.SGD(netIE.parameters(), lr=lr, momentum= 0.9)  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    losses = []\n",
    "    for i, (image, labels) in enumerate(dataloader):  #<----take in labels\n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        cur_batch_size = image.shape[0]\n",
    "\n",
    "        latent, output = netIE(image)  #output shape: (batch, num_classes) #put underscore to ignore latent vector\n",
    "        loss = criterion(output, labels)  #labels shape: (batch, )  #cross entropy loss handles the encoding\n",
    "        \n",
    "        optimizerIE.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizerIE.step()\n",
    "                \n",
    "        # Save Losses for plotting later\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Predictions (no need to detach() since it does not need to go backward again)\n",
    "        _, y_pred = torch.max(output.data, 1)  #ignore the max tensor\n",
    "        total += labels.size(0)\n",
    "        correct += (y_pred == labels).sum().item()\n",
    "        \n",
    "    sys.stdout.write(\"\\rEpoch: \" + str(epoch) + \"; Accuracy: \" + str((100 * correct) / total) + \"; Loss: \" + str(sum(losses) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "Follow the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.net = nn.Sequential(\n",
    "            #Input: batch x z_dim x 1 x 1 (see definition of noise in below code)\n",
    "            self._block(z_dim + embed_size + feature_size, gen_dim * 16, 4, 1, 0),  #batch x 1024 x 4 x 4\n",
    "            self._block(gen_dim * 16, gen_dim * 8, 4, 2, 1),  #batch x 512 x 8 x 8\n",
    "            self._block(gen_dim * 8, gen_dim * 4, 4, 2, 1),  #batch x 256 x 16 x 16\n",
    "            self._block(gen_dim * 4, gen_dim * 2, 4, 2, 1),  #batch x 128 x 32 x 32\n",
    "            nn.ConvTranspose2d(\n",
    "                gen_dim * 2, num_channel, kernel_size = 4, stride = 2, padding =1,  #did not use block because the last layer won't use batch norm or relu\n",
    "            ),  #batch x 3 x 64 x 64\n",
    "            nn.Tanh(), #squeeze output to [-1, 1]; easier to converge.  also will match to our normalize(0.5....) images  \n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, embed_size)\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels,kernel_size,stride,padding,bias=False,  #batch norm does not require bias\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True) #in_place = True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels, semantic_latent):\n",
    "        #semantic latent: batch, feature_size\n",
    "        #Input: latent vector z: batch x z_dim x 1 x 1\n",
    "        #in order to concat labels with the latent vector, we have to create two more dimensions of 1 by unsqueezing\n",
    "        semantic_latent = semantic_latent.unsqueeze(2).unsqueeze(3) #batch, feature_size, 1, 1\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3) #batch, embed_size, 1, 1\n",
    "        x = torch.cat([x, embedding, semantic_latent], dim=1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can instantiate the generator and apply the ``weights_init`` function. Check out the printed model to see how the generator object is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (net): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(400, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (embed): Embedding(4, 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "Follow the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.net = nn.Sequential(\n",
    "            #no batch norm in the first layer \n",
    "            #Input: batch x num_channel x 64 x 64\n",
    "            #<-----changed num_channel + 1 since we add the labels\n",
    "            nn.Conv2d(\n",
    "                num_channel + 1, dis_dim, kernel_size=4, stride=2, padding=1,\n",
    "            ), #batch x 64 x 32 x 32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self._block(dis_dim, dis_dim * 2, 4, 2, 1), #batch x 128 x 16 x 16\n",
    "            self._block(dis_dim * 2, dis_dim * 4, 4, 2, 1), #batch x 256 x 8 x 8\n",
    "            self._block(dis_dim * 4, dis_dim * 8, 4, 2, 1), #batch x 512 x 4  x 4\n",
    "            nn.Conv2d(dis_dim * 8, 1, kernel_size=4, stride=2, padding=0), #batch x 1 x 1 x 1 for classification\n",
    "#             nn.Sigmoid(), #<------removed!\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, image_size*image_size)\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,  #batch norm does not require bias\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True), #<----changed here\n",
    "            nn.LeakyReLU(0.2, True) #slope = 0.2, in_place = True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        #Label shape: batch, \n",
    "        #Label after embed shape: batch, image_size * image_size\n",
    "        #reshape the labels further to be of shape (batch, 1, H, W) so we can concat\n",
    "        #embedding shape:  batch, 1, image_size, image_size\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, image_size, image_size)\n",
    "        x = torch.cat([x, embedding], dim = 1) #batch x (C + 1) x W x H\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as with the generator, we can create the discriminator, apply the\n",
    "``weights_init`` function, and print the model’s structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(4, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
      "  )\n",
      "  (embed): Embedding(4, 4096)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "    \n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 400, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#testing embedding\n",
    "def test():\n",
    "    batch = 47\n",
    "\n",
    "    semantic_latent = latent.unsqueeze(2).unsqueeze(3).cpu()\n",
    "    semantic_latent.shape #47, 200, 1, 1\n",
    "\n",
    "    labels = torch.randint(0, 4, (batch,), dtype=torch.float)\n",
    "    labels = labels.long()\n",
    "    embed = nn.Embedding(num_classes, embed_size)\n",
    "    embedding = embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "    embedding.shape #47, 100, 1, 1\n",
    "\n",
    "    z = torch.randn((batch, z_dim, 1, 1))\n",
    "    z.shape #47, 100, 1, 1\n",
    "\n",
    "    combined = torch.cat([z, embedding, semantic_latent], 1)\n",
    "    print(combined.shape)\n",
    "    \n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's write a test function to test whether our dis and gen are correctly coded\n",
    "def test():\n",
    "    batch, num_channels, H, W = 47, num_channel, 64, 64\n",
    "    z_dim = 100\n",
    "    x = torch.randn((batch, num_channels, H, W))\n",
    "    labels = torch.randint(0, num_classes, (batch,), dtype=torch.float)\n",
    "    labels = labels.long() #<----pytorch expect the labels to be long\n",
    "    \n",
    "    #testing discriminator\n",
    "    netD = Discriminator(ngpu)\n",
    "    netD.apply(weights_init)\n",
    "    assert netD(x, labels).shape == (batch, 1, 1, 1) #output of dis should be only one value, ready for classification\n",
    "    \n",
    "    #testing generator\n",
    "    netG = Generator(ngpu)\n",
    "    netG.apply(weights_init)\n",
    "    z = torch.randn((batch, z_dim, 1, 1))  #input to gen is (batch, 100, 1, 1)\n",
    "    latent_cpu = latent.cpu()\n",
    "    assert netG(z, labels, latent.cpu()).shape == (batch, num_channels, H, W) #output of gen is (batch, 3, 64, 64)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions and Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "# criterion = nn.BCELoss() <-----removed\n",
    "\n",
    "# Create 64 latent vectors that we will use to visualize\n",
    "# the progression of the generator\n",
    "# fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)  #<---no need since we need noise with labels, so we just gonna use noise to monitor progress\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "# Use momentum of 0.5 following the paper\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.0, 0.9))  #<-----changed back! to RMSprop\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.0, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforces the gradient to be close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(dis, labels, real, fake, device=\"cpu\"): #<---add labels\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate dis scores\n",
    "    mixed_scores = dis(interpolated_images, labels)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "-  **Loss_D** - discriminator loss calculated as the sum of losses for\n",
    "   the all real and all fake batches ($log(D(x)) + log(D(G(z)))$).\n",
    "-  **Loss_G** - generator loss calculated as $log(D(G(z)))$\n",
    "-  **D(x)** - the average output (across the batch) of the discriminator\n",
    "   for the all real batch. This should start close to 1 then\n",
    "   theoretically converge to 0.5 when G gets better. \n",
    "-  **D(G(z))** - average discriminator outputs for the all fake batch.\n",
    "   The first number is before D is updated and the second number is\n",
    "   after D is updated. These numbers should start near 0 and converge to\n",
    "   0.5 as G gets better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'semantic_latent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-640523e5266f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moutput_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0moutput_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'semantic_latent'"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real, labels) in enumerate(dataloader):  #<----take in labels\n",
    "        real = real.to(device)\n",
    "        labels = labels.long().to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        #Train D min - (E[dis(real)] - E[dis(fake)])\n",
    "        for _ in range(dis_iter):\n",
    "            output_real = netD(real, labels).view(-1)   \n",
    "            noise = torch.randn(cur_batch_size, z_dim, 1, 1, device=device)\n",
    "            fake = netG(noise, labels)\n",
    "            output_fake = netD(fake, labels).view(-1)\n",
    "            gp = gradient_penalty(netD, labels, real, fake, device=device)\n",
    "            dis_loss = (\n",
    "                -(torch.mean(output_real) - torch.mean(output_fake)) + lambda_gp * gp\n",
    "            )\n",
    "            netD.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True) #similar to detach()\n",
    "            optimizerD.step()\n",
    "                                \n",
    "        # Train G: min -E[dis(gen_fake)]\n",
    "        output = netD(fake, labels).view(-1)\n",
    "        gen_loss = -torch.mean(output)        \n",
    "        netG.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        sys.stdout.write(\"\\r\" + str(epoch))\n",
    "        \n",
    "#         # Output training stats\n",
    "#         if i % 50 == 0:\n",
    "#             print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "#                   % (epoch, num_epochs, i, len(dataloader),\n",
    "#                      errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(gen_loss.item())\n",
    "        D_losses.append(dis_loss.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(noise, labels).detach().cpu() #<---send in noise instead to make sure the label matches\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "**Loss versus training iteration**\n",
    "\n",
    "Below is a plot of D & G’s losses versus training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of G’s progression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real Images vs. Fake Images**\n",
    "\n",
    "Finally, lets take a look at some real images and fake images side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
